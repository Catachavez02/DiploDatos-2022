# -*- coding: utf-8 -*-
"""lab2_stable_baselines_para_entregar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ATbToUfqcCrwqP8OgV3vRp1YamnNLCld

# Trabajo Práctico Nº 2

* Chediack, Camila
* Borghello, Fernanda 
* Chávez, Lourdes Catalina

# Introducción

Créditos:

* Documentación y repo de Stable-baselines https://stable-baselines3.readthedocs.io.
    * Tutorial sobre SB3: https://github.com/araffin/rl-tutorial-jnrr19.
* Documentación y repo de OpenAI Gym https://github.com/openai/gym/blob/master/docs/.
    * Crear un entorno https://github.com/openai/gym/blob/master/docs/creating-environments.md.

Stable-baselines3: framework de deep RL que provee interfaces para ejecutar y adaptar algoritmos de RL "al estilo scikit-learn". Permite utilizar agentes abstrayéndonos de los detalles de bajo nivel de abstracción referentes a la implementación del algoritmo$^1$

Además, ofrece herramientas muy útiles como

* Monitores que permiten ver el rendimiento del agente según se desempeña en el entorno, sin tener que esperar a que finalice de entrenar.
* Callbacks que permiten accionar eventos cuando se cumplen algunas condiciones en el entrenamiento de nuestro agente (por ejemplo, detenerlo si la recompensa recibida es menor a cierto umbral tras un cierto período de tiempo).


Documentación https://stable-baselines3.readthedocs.io

Es un fork activamente mantenido de [OpenAI baselines](https://github.com/openai/baselines)

La versión 3 cambia el framework subyacente de Tensorflow a Pytorch y está activamente en desarrollo; no obstante la versión 2 es completamente funcional

$^1$ no obstante, al igual que sucede generalmente con librerías de ML: 

* Siempre es bueno tener en mente las características, ventajas y desventajas del algoritmo utilizado, pues de eso depende mucho la convergencia de nuestra solución, especialmente cuando se emplean entornos adaptados para nuestras necesidades. 

* Esta librería, al igual que demás frameworks generales de RL, están muy probadas en entornos estándares de RL como Atari o PyBullet. No obstante, es posible que nuestro entorno o nuestras necesidades difieran significativamente, lo que hace que en algunos casos haya que meter mano directo en el código de los algoritmos/librería.

# Interfaz básica stable-baselines

### Instalación de Stable-baselines

Desde Linux o Google Colab
"""

#@title Instalación (no modificar)
!pip install stable-baselines3[extra,tests,docs]>=0.11.0a4 && pip install sb3-contrib

"""Desde Windows, además, instalar: 
* Microsoft Visual C++ desde https://visualstudio.microsoft.com/visual-cpp-build-tools/
* PyType, mediante `conda install -c conda-forge pytype`

### Instalación de RLBaselinesZoo

Desde Google Colab
"""

#@title Instalación de RLBaselinesZoo (no modificar)

#if IN_COLAB:
!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo
!cd rl-baselines3-zoo/
!apt-get install swig cmake ffmpeg
!pip install -r /content/rl-baselines3-zoo/requirements.txt

"""Desde Linux, ejecutando

    git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo
    cd rl-baselines3-zoo/
    sudo apt-get install swig cmake ffmpeg
    pip install -r requirements.txt

### Instalación desde Nabucodonosor

    wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh
    chmod 755 Anaconda3-2020.11-Linux-x86_64.sh
    ./Anaconda3-2020.11-Linux-x86_64.sh
    conda create --name rl
    conda activate rl
    conda config --add channels conda-forge
    conda install jupyter atari_py swig
    pip install stable-baselines3[extra,tests,docs]>=0.11.0a4
    pip install sb3-contrib

## Ejecución de un algoritmo de RL

### Importaciones/inicializaciones
"""

# Commented out IPython magic to ensure Python compatibility.
import os
from subprocess import Popen, PIPE

import numpy as np
import matplotlib.pyplot as plt

import gym
from gym import spaces
#from gym.envs.registration import register

from stable_baselines3 import DQN, PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold
from stable_baselines3.common.env_util import make_vec_env

os.makedirs('logs', exist_ok=True)

try:
  import google.colab
  IN_COLAB = True
except:
  IN_COLAB = False

cwd = os.getcwd()

# %matplotlib inline
# %load_ext tensorboard

"""### Ejemplo básico"""

env = gym.make('CartPole-v1')

# MlpPolicy es una política "estándar" que aprende con perceptron multicapa
# (es decir sin capas convolucionales o demás variantes),
# 2 capas ocultas con 64 neuronas cada una
model = DQN('MlpPolicy', env)
model.learn(total_timesteps=10000)

"""### Renderización"""

if not IN_COLAB:

    obs = env.reset()
    for i in range(1000):
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, done, info = env.step(action)
        env.render()
        if done:
          obs = env.reset()

    env.close()

"""### Logging

#### Ver rendimiento del agente en tensorboard
"""

venv = make_vec_env(lambda: gym.make('CartPole-v1'), n_envs=1)

model = DQN('MlpPolicy', venv, tensorboard_log='tensorboard/')
model.learn(total_timesteps=100000)

"""Para verlo en tensorboard, correr

`tensorboard --logdir=tensorboard/`
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=tensorboard

"""### Monitor

Vamos a crear un monitor para loguear nuestro agente en la carpeta logs. Nuestro monitor guardará datos de recompensa (r), duración (l) y tiempo total (t)
"""

env = gym.make('CartPole-v1')
env = Monitor(env, 'logs/')  # reemplazamos env por su monitor

model = DQN('MlpPolicy', env, )
model.learn(total_timesteps=10000)

"""### Callbacks"""

env = gym.make('CartPole-v1')

callbacks = []  # lista de callbacks a usar, pueden ser varios

# callback para detener entrenamiento al alcanzar recompensa de 9.8
# (a fines demostrativos, es una recompensa baja)
stop_training_callback = StopTrainingOnRewardThreshold(reward_threshold=9.8)

# al crear EvalCallback, se asocia el mismo con stop_training_callback
callbacks.append(EvalCallback(env, 
                              eval_freq=1000,
                              callback_on_new_best=stop_training_callback))

# la semilla aleatoria hace que las ejecuciones sean determinísticas
model = DQN('MlpPolicy', env, seed=42)
model.learn(total_timesteps=10000, callback=callbacks)

"""### Ejecutar agente RL en múltiples ambientes

Esta librería provee una interfaz para ejecutar agentes en varias instancias de un mismo entorno a la vez (*vectorized environments*), de modo tal que se habilite la ejecución paralela y de otras funcionalidades útiles.

Para ello, varios de sus algoritmos implementan cambios que consideren la posibilidad de que haya múltiples entornos subyacentes, por ejemplo `step(accion)` cambia a `step(lista_acciones)`, aplicando acciones a todos los entornos, recibiendo ahora múltiples observaciones y recompensas.

Otro cambio: se aplica `reset()` automáticamente a cada entorno que llega a un estado final.

SB brinda dos formas de utilizar entornos vectorizados:

* **DummyVecEnv**, el cuál consiste en un *wrapper* de varios entornos, los cuáles funcionarán en un sólo hilo. Este wrapper es útil como entrada de algoritmos que requieren los entornos de esta forma, y habilita los procesamientos y operaciones comunes de los entornos vectorizados.
* **SubprocVecEnv**, el cuál paraleliza multiples entornos pero en procesos de ejecucíon separados. Cada proceso tiene su propia memoria y puede adquirir derechos sobre las CPUs de la computadora donde se ejecuta. Se utiliza cuando el entorno del agente es computacionalemente complejo. Atención! **Puede comer mucha RAM**.

Vemos un ejemplo:
"""

# ejemplo de ambiente dummy
venv = DummyVecEnv([lambda: gym.make('CartPole-v1')]*4)

model = PPO('MlpPolicy', venv, )
model.learn(total_timesteps=10000)

"""También puede hacerse con un una función de SB a tal efecto"""

venv = make_vec_env(lambda: env, n_envs=4)

model = PPO('MlpPolicy', venv, )
model.learn(total_timesteps=10000)

"""### Ejecutar agente con políticas personalizadas"""

# Creamos una clase con una red neuronal de 128x128 neuronas

model = PPO('MlpPolicy', policy_kwargs=dict(net_arch=[128,128]), env='CartPole-v1', verbose=1)
model.learn(total_timesteps=10000)

"""### Utilizar un entorno personalizado

Antes que nada, además de la interfaz que ya vimos de Gym, hay otras nociones que tenemos que tener en cuenta en este contexto:

* Los entornos definen un espacio de estados y de acciones, a partir de los cuáles los modelos asumen y respetan la "forma" de observaciones y acciones. Por ejemplo, algunos algoritmos están diseñados para espacios de acciones discretos (DQN), continuos (DDPG) o bien poseen implementaciones particulares pueden usarse en ambos (PPO, en el repo de SB3). En cuanto a los espacios, algunos algoritmos asumen explícitamente un espacio discreto (y pequeño), como Q-Learning, mientras que otros como PPO asumen cualquier tipo de espacio.
* Los dos tipos más comunes de estados o acciones son los espacios discretos `gym.spaces.Discrete` y los continuos `gym.spaces.Box`.
* Los espacios discretos definen un conjunto de $n$ estados/acciones $\{ 0, 1, \dots, n-1 \}$, mientras que los espacios continuos definen un espacio $\mathbb{R}^d$, de una de las siguientes 4 formas: $[a, b], (-\infty, b], [a, \infty), (-\infty, \infty)$, en donde $a,b$ son las cotas superior e inferior (de existir).
* Ejemplos: un espacio de acciones `Discrete(4)` tiene 4 acciones: $\{0,1,2,3\}$; un espacio de estados `Discrete(16)` tiene 16 estados. Un espacio de estados ALTURA, ANCHO, N_CANALES que represente una imagen RGB acotada en $[a=0, b=255]$ se puede crear como

`observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)`

Para usar un entorno compatible por esta librería, el mismo tiene que heredar de *gym.Env*. Vemos un ejemplo (crédito: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)
"""

class GoLeftEnv(gym.Env):
  """
  Ambiente personalizado que sigue la interfaz de gym.
  Es un entorno simple en el cuál el agente debe aprender a ir siempre 
  hacia la derecha.
  """
  # Dado que estamos en colab, no podemos implementar la salida por interfaz 
  # gráfica ('human' render mode) 
  metadata = {'render.modes': ['console']}
  # Definimos las constantes
  LEFT = 0
  RIGHT = 1

  def __init__(self, grid_size=10):
    super(GoLeftEnv, self).__init__()

    # Tamaño de la grilla de 1D
    self.grid_size = grid_size
    # Inicializamos en agente a la derecha de la grilla
    self.agent_pos = grid_size - 1

    # Definimos el espacio de acción y observaciones
    # Los mismos deben ser objetos gym.spaces
    # En este ejemplo usamos dos acciones discretas: izquierda y derecha
    n_actions = 2
    self.action_space = spaces.Discrete(n_actions)
    # La observación será la coordenada donde se encuentra el agente
    # puede ser descrita tanto por los espacios Discrete como Box
    self.observation_space = spaces.Box(low=0, high=self.grid_size,
                                        shape=(1,), dtype=np.float32)

  def reset(self):
    """
    Importante: la observación devuelta debe ser un array de numpy
    :return: (np.array) 
    """
    # Se inicializa el agente a la derecha de la grilla
    self.agent_pos = self.grid_size - 1
    # convertimos con astype a float32 (numpy) para hacer más general el agente
    # (en caso de que querramos usar acciones continuas)
    return np.array([self.agent_pos]).astype(np.float32)

  def step(self, action):
    if action == self.LEFT:
      self.agent_pos -= 1
    elif action == self.RIGHT:
      self.agent_pos += 1
    else:
      raise ValueError("Received invalid action={} which is not part of the action space".format(action))

    # Evitamos que el agente se salga de los límites de la grilla
    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)

    # Llegó el agente a su estado objetivo (izquierda) de la grilla?
    done = bool(self.agent_pos == 0)

    # Asignamos recompensa sólo cuando el agente llega a su objetivo
    # (recompensa = 0 en todos los demás estados)
    reward = 1 if self.agent_pos == 0 else 0

    # gym también nos permite devolver información adicional, ej. en atari: 
    # las vidas restantes del agente (no usaremos esto por ahora)
    info = {}

    return np.array([self.agent_pos]).astype(np.float32), reward, done, info

  def render(self, mode='console'):
    if mode != 'console':
      raise NotImplementedError()
    # en nuestra interfaz de consola, representamos el agente como una cruz, y 
    # el resto como un punto
    print("." * self.agent_pos, end="")
    print("x", end="")
    print("." * (self.grid_size - self.agent_pos))

  def close(self):
    pass

env = GoLeftEnv(grid_size=10)
env = make_vec_env(lambda: env, n_envs=1)

model = PPO('MlpPolicy', env, verbose=1).learn(20000)

"""Ver agente entrenado"""

obs = env.reset()
for i in range(10):
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    env.render(mode='console')
    if dones[0]:
        obs = env.reset()

"""## Normalización de features y recompensas

Stable-Baselines3 tiene predefinidos un conjunto de **wrappers** genericos que pueden utilizarse para preprocesar las observaciones que llegan al agente RL, desacomplando del mismo el prepocesamiento.

Entre las funcionalidades disponibles tenemos:
* **VecFrameStack**: Se utiliza cuando la observación que percibe el agente es una imagen. Sirve para expandir el espacio de estados apilando N frames de manera conjunta.
* **VecNormalize**: Se utiliza para normalizar las observaciónes y/o las recompenzas que percibe el agente a $\mu=0$ y $\sigma=1$. También permite cortar valores de observaciones y/o recompensas que excedan un rango establecido. 
* **VecCheckNan**: Se utiliza para trackear los estados del entorno que generan que los gradientes de la RNN se hagan NaN.
* **VecVideoRecorder**: Se utiliza para exportar el funcionamiento de la política aprendida por el agente a un video (MP4).

Ademas, se pueden crear **wrappers** personalizados extendiendo la clase **VecEnvWrapper**:

```
class MiWrapper(VecEnvWrapper):
    [...]
```
### Ejemplo (VecNormalize)
"""

from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

env = gym.make('CartPole-v1')
env = DummyVecEnv([lambda: env])  # Multiple vectorize environments
# Observations and reward normalization
env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)

model = DQN('MlpPolicy', env)
model.learn(total_timesteps=10000)

"""# RL-baselines3-zoo

Colección de agentes RL y herramientas útiles para ejecutarlos, evaluarlos e incluso hacer videos con ellos. Los agentes de este repo están preparados con la configuración requerida para los distintos tipos de entornos, incluyendo Atari, PyBullet y entornos clásicos, incluyendo configuraciones e híper-parámetros que producen buenas políticas para tales entornos.

Esta librería ofrece un muy buen punto de partida para utilizar agentes / entornos personalizados, ya que ofrece una [interfaz](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/train.py) fácilmente adaptable a nuestras necesidades.

Si se usan entornos personalizados con rl-baselines3-zoo, debe tenerse en cuenta que se deben definir todos los híper-parámetros de antemano sea al instanciar el agente o en la carpeta /rl-baselines3-zoo/hyperparams; de lo contrario arrojará error por no encontrar qué híper-parámetro usar.

## Ejecución

Los agentes pueden ser llamados desde la consola mediante comandos como

`python train.py --algo algo_name --env env_id`

Los cuales pueden ser llamados usando
"""

os.chdir('rl-baselines3-zoo/')

args = [
    '-n', str(100000),
    '--algo', 'ppo',
    '--env', 'CartPole-v1'
]

p = Popen(['python', 'train.py'] + args,
                               stdin=PIPE, stdout=PIPE, stderr=PIPE)
output, err = p.communicate()
rc = p.returncode
os.chdir(cwd)
assert rc == 0

"""Ver en acción el agente entrenado (nota: no disponible en Google Colab, requiere ffmpeg)"""

if not IN_COLAB:
    os.chdir('rl-baselines3-zoo/')

    args = [
        '--algo', 'ppo',
        '--env', 'CartPole-v1',
        '--folder', 'logs/'
    ]

    p = Popen(['python', 'enjoy.py'] + args,
                                   stdin=PIPE, stdout=PIPE, stderr=PIPE)
    output, err = p.communicate()
    rc = p.returncode
    os.chdir(cwd)
    assert rc == 0

"""También es posible grabar un video! Ver https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#record-a-video

Ver curva de aprendizaje obtenida por el agente desde *utils.plot*
"""

os.chdir('rl-baselines3-zoo/')

args = [
    '--algo', 'ppo',
    '--env', 'CartPole-v1',
    '--exp-folder', 'logs/'
]

p = Popen(['python', '-m', 'scripts.plot_train'] + args, stdout=PIPE)
output, err = p.communicate()
rc = p.returncode
os.chdir(cwd)

assert rc == 0

print(output)

"""## Híper-parámetros

rl-baselines-zoo provee híper-parámetros que resultan en curvas de aprendizaje que convergen en buena cantidad de entornos. Estos híper-parámetros pueden verse en cada uno de los archivos YAML de cada algoritmo, [acá](https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams).

También provee funcionalidad para optimizar los híper-parámetros con la librería [Optuna]( https://github.com/optuna/optuna). En los mismos se incluyen rangos de híper-parámetros que se usaron para optimizar entornos como los de PyBullet, y son fácilmente modificables para adaptarlo a nuestros propios entornos. Para ver cómo se llama a la interfaz de Optuna ver [este código](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py).

Nota: **Optuna come muchos recursos!**

# Entrenando otros agentes

### Algunos conjuntos de entornos

CartPole es una excelente línea base (de hecho suele ser la prueba preliminar de todo nuevo algoritmo), porque tiene recompensas constantes pero requiere cierta solidez por parte del algoritmo para hacerlo converger al óptimo.

No obstante, al implementar un algoritmo, es deseable que el mismo pueda desenvolverse de forma consistente en varios grupos de entornos. A continuación va una lista con varios entornos que sirven como prueba:

| Entornos                                                                                                           | Estados            | Acciones            | Dificultad      | Implementado por                                                                                                                                                            |
|--------------------------------------------------------------------------------------------------------------------|--------------------|---------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Clásicos (CartPole, MountainCar, Pendulum, Deep sea (testea exploración), umbrella (testea asignación de crédito)) | Discretos/Continuos          | Continuas/discretas | Baja/Media      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) y [BSuite](https://github.com/deepmind/bsuite)                                                                                                                          |
| Grilla (desde pequeñas donde hay que salir hasta grandes con muchas habitaciones y subproblemas)                   | Discretos/imágenes | Discretas           | Baja/Media/Alta | [gym-minigrid](https://github.com/maximecb/gym-minigrid)                                                                                                                    |
| Grilla/plataforma/estilo Atari, generados proceduralmente                                                          | Imágenes           | Discretas           | Media/Alta      | [Gym](https://github.com/openai/procgen)                                                                                                                                    |
| Plataforma 2D, como LunarLander o BipedalWalker                                                                    | Continuos          | Continuas/discretas | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments)                                                                                                                                                                         |
| Primera persona en 3D                                                                                              | Imágenes           | Continuas/discretas | Media/Alta      | [Deepmind](https://github.com/deepmind/lab)                                                                                                                                 |
| Simulación física de pequeños robots                                                                               | Continuos          | Continuas           | Media/Alta      | Gym (con el motor MuJoCo o su versión open-source, [PyBullet](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.wz5to0x8kqmr)) |
| Atari                                                                                                              | Continuos          | Discretas           | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) (son todos aquellos entornos que terminan en "*-v4*"                                                                                                                                                                         |

## Resumen de algunos algoritmos

Se resumen ahora varios algoritmos del estado del arte de aprendizaje por refuerzos profundo

| Algoritmo | Tipo       | Espacio de acciones | Resumen rápido                                                                                                                                                | Artículo                         |
|-----------|------------|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|
| DQN       | Off-policy | Discretas           | Extiende Q-Learning a deep learning. En Stable-baselines, DQN incluye todas las mejoras ya incorporadas.                                                      | https://arxiv.org/abs/1312.5602  |
| ACER      | Off-policy | Discretas           | Combina una arquitectura actor-critic con un buffer y repetición de experiencia.                                                                              | https://arxiv.org/abs/1611.01224 |
| A3C       | On-policy  | Ambos               | Múltiples agentes corriendo en múltiples instancias del ambiente, acumulando sus gradientes y actualizándolos tras un cierto tiempo.                          | https://arxiv.org/abs/1602.01783 |
| PPO       | On-policy  | Ambos               | Punta de lanza de policy gradient, incluye mecanismo para que el gradiente actualice de forma acotada, mejorando drásticamente la estabilidad.             | https://arxiv.org/abs/1707.06347 |
| DDPG      | Off-policy | Continuas           | Como en espacios de acciones continuos es muy difícil encontrar $\max_a Q(s,a)$, se aproxima via $Q(s, a(s \mid \theta_a))$, siendo $a$ un actor estocástico. | https://arxiv.org/abs/1509.02971 |
| TD3       | Off-policy | Continuas           | Mejora DDPG utilizando dos funciones $Q$ y retrasando la actualización para reducir la sobreestimación de $Q$.                                                | https://arxiv.org/abs/1802.09477 |
| SAC       | Off-policy | Continuas           | Usa dos funciones $Q$, introduce el bonus por entropía y usa un actor estocástico que muestrea acciones según una política $\pi$.                             | https://arxiv.org/abs/1801.01290 |

## Resumen de algunas herramientas/trucos comúnmente usados

* Experience replay/buffer de experiencia: guarda las experiencias en un buffer para poder usarlas repetidamente durante el entrenamiento.
    * Ventajas: experiencias raras pero muy relevantes (ej: que tienen mucho error de actualización) quedan guardadas en memoria, pudiendo ser usadas repetidamente para aprender sin necesidad de esperar a que se repitan.
    
    En métodos de gradiente de política, en cambio, el aprendizaje queda reflejado en los pesos, lo cuál puede hacer que un agente no se desenvuelva correctamente en entornos de recompensa escasa como MountainCar.
    
    * Desventajas: requiere considerable RAM, son usables solamente en algoritmos off-policy y su muestreo no necesariamente refleja la probabilidad real de tener esas experiencias en el entorno (añadiendo sesgo), por lo que es recomendable usarlo junto con importance sampling.
    
* Importance sampling: aplica un descuento a las actualizaciones a partir de experience replay relacionado a cuán probable era realizar esa transición.
* Entropía: añade un bonus a la función de recompensa para que bonifique políticas $\pi(s \mid a)$ que tengan mayor entropía que otras, motivando la exploración del agente.
* Juntar varias secuencias de imágenes. Usado principalmente en entornos de Atari para poder evaluar la dirección de movimientos.
* Normalización de recompensas/estados. Normaliza las recompensas y observaciones usualmente con una media móvil, de modo tal que las observaciones/recompensas reflejen su relación y varianza con respecto a las demás.
* Clipping (recorte) de recompensas. Se usaba principalmente en entornos de Atari con algoritmos tipo DQN para recortar el impacto que las distintas recompensas tenían, a una constante (ej: 1). Suele usarse como una cota máxima de recompensas/observaciones normalizadas.

# Lab 2

1. Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e híper-parámetros. 

    Algunas ideas:

    * Transformar GoLeftEnv en una grilla 2D, añadir paredes / trampas / agua.
    * Crear un entorno que juegue a algún juego como el ta-te-ti.
    * Crea un entorno totalmente nuevo que sea de tu interés!

2. Entrena agentes en entornos más complejos con stable-baselines/rl-baselines-zoo. Tener en cuenta:

    * Google Colab tiene una limitante en cuanto a cantidad de recursos de CPU/GPU (incluido un "rendimiento decreciente silencioso"), lo cuál reduce la capacidad de entrenar distintos entornos.
    * Si el entorno no está implementado en stable-baselines, debe hacerse un wrapper a mano, lo que puede ser sencillo o puede llevar algo más de trabajo, teniendo que tocar código subyacente de la librería. 

\* pueden ser usando stable-baselines/rl-baselines-zoo o bien utilizando algún otro algoritmo (incluso tabular)

# Recursos adicionales

* [Excelente recurso para aprender más de deep RL](https://spinningup.openai.com/en/latest/spinningup/spinningup.html)
* [Framework adicional de aprendizaje por refuerzos a gran escala](https://docs.ray.io/en/master/rllib.html)
* [Awesome Deep RL](https://github.com/kengz/awesome-deep-rl)
* [Comunidad de Bots de RL para Rocket League](https://rlbot.org/)
* [Blog de Lilian Weng de Deep RL, robótica y NLP](https://lilianweng.github.io/lil-log/)
* [The Nuts and Bolts of Deep RL Research](http://joschu.net/docs/nuts-and-bolts.pdf)

FIN

## Punto 1 y 2

#Lunar Lander (Aterrizaje Lunar)

Lunar Lander es  un entorno de tipo Box 2D. Consiste en una nave espacial con tres aceleradores cuyo objetivo es aterrizar correctamente siguiendo el camino marcado por 2 banderas situadas en el suelo. 


#### Recompensas: 
* (+)100 puntos si la nave aterriza correctamente. 
* (-)100 puntos si la nave no aterriza correctamente(se estrella).
* 0.3 puntos cada vez que se activa el motor inferior.
* 0.03 puntos cada vez que se activa uno de los motores laterales.
* 10 puntos contacto de cada pata con tierra.

##### Motivos de finalización del episodio:
* La nave aterriza.
* La nave se estrella.

**En este caso, no existe un límite de puntuación máxima, pero se considera una victoria a partir de 200 puntos.**

## Algoritmo DQN (Deep Q Network)
"""

#importar librerias 
import warnings
warnings.filterwarnings("ignore")
import gym
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.td3.policies import MlpPolicy
from stable_baselines3.common.evaluation import evaluate_policy

# Crear el entorno
env = gym.make('CarRacing-v2')
env = DummyVecEnv([lambda: env])

model = DQN('MlpPolicy', env, exploration_final_eps=0.1, target_update_interval=250, tensorboard_log='tensorboard/')

# Valores de recompensa antes de entrenar el agente
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)

print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")

# Entrenar el agente 
model.learn(total_timesteps=100000)
# Salvar el agente
model.save("dqn_lunar")
del model

# Cargar el agente entrenado
model = DQN.load("dqn_lunar")
# Valores de recompensa luego de entrenar el agente
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)

print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")

# Commented out IPython magic to ensure Python compatibility.
# Visualizamos en tensorboard
# %tensorboard --logdir=tensorboard

"""## Algoritmo PPO (Proximal Policy Optimization)"""

# Crear el entorno
env2 = gym.make('LunarLander-v2')
env2 = DummyVecEnv([lambda: env2])

model = PPO('MlpPolicy', env2, tensorboard_log='tensorboard/')
# Valores de recompensa antes de entrenar el agente
mean_reward, std_reward = evaluate_policy(model, env2, n_eval_episodes=10, deterministic=True)

print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")

# Entrenar el agente
model.learn(total_timesteps=100000)
# Guardar el agente
model.save("ppo_lunar")
del model  # delete trained model to demonstrate loading

# Cargar el agente entrenado
model = PPO.load("ppo_lunar")

# Valores de recompensa luego de entrenar el agente
mean_reward, std_reward = evaluate_policy(model, env2, n_eval_episodes=10, deterministic=True)

print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=tensorboard

"""## Algoritmo A2C """

from stable_baselines3 import A2C

# Crear el entorno
env3 = gym.make('LunarLander-v2')
env3 = DummyVecEnv([lambda: env3])
model = A2C('MlpPolicy', env3, tensorboard_log='tensorboard/')

# Valores de recompensa antes de entrenar el agente
mean_reward, std_reward = evaluate_policy(model, env2, n_eval_episodes=10, deterministic=True)

print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")

# Entrenar el agente
model.learn(total_timesteps=100000) 
# Guardar el agente
model.save("a2c_lunar")
del model

# Cargar el agente entrenado
model = A2C.load("a2c_lunar")
# Valores de recompensa luego de entrenar el agente
mean_reward, std_reward = evaluate_policy(model, env3, n_eval_episodes=10, deterministic=True)

print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=tensorboard

"""## **Conclusión:**
En los experimentos realizados anteriormente con el entorno **Lunar Lander** y utilizando los algoritmos **DQN**, **PPO** y **AC2** con parámetros por defecto, es posible observar a simple vista cómo mejora el aprendizaje del agente si comparamos los valores de recompensa promedio obtenidos antes y después del entrenamiento del mismo, tal como se observa en la siguiente tabla:

|       | Antes del entrenamiento del agente | Después del entrenamiento del agente |
| ------|-------------------| -----------------|
| Valor promedio con DQN    | -593.39| 59.18   |
| Valor promedio con PPO    | -319.99 | -147.1 |
| Valor promedio con AC2    | -838.17  | 7.24  |


Luego de analizar los valores de recompensa y las gráficas generadas con tensorboard al finalizar el entrenamiento con los tres algoritmos anteriormente mencionados, se concluye que el mejor valor de recompensa se obtiene con el algoritmo DQN, con valor promedio de recompensa igual a + 59.18.

## **Fuentes:**

https://stable-baselines.readthedocs.io/en/v2.4.0/guide/examples.html

https://towardsdatascience.com/stable-baselines-a-fork-of-openai-baselines-reinforcement-learning-made-easy-df87c4b2fc82 

https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/saving_loading_dqn.ipynb
"""